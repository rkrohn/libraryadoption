6.11 notes

how to train a model off this data? regression?

every commit is an instance, what are our feature vectors?
	sometimes an adoption happens, sometimes not
	
StackOverflow - 16 million questions, 25 million answers
actually, ~40 million posts - according to Tim
	
User Features - specific to one particular user
*****
# packages commited - half-life?
# packages implicitly seen
time since last commit
time since last adoption
intra-commit duration for last 10% of commits
# repositories commited to
# repositories commited to in last 10% of commits
% commits with adoptions
% commits with imports
% commits with adoptions within last 10% of commits
% commits with imports within last 10% of commits
# implicitly seen package i
# implicitly seen package i within last 10% of commits
# implicitly seen package i / total # implicit packages seen
# implicitly seen package i within last 10% commits / total # implicit packages seen within last 10% commits
{packages implicitly seen}

Package Features - specific to one particular package 
*****
# of addition commits
# users who have committed
# users who have adopted (# adoptions)
# repos containing package
|{u committed packages} ^ {U_(x in U ^ x adopted i) all committed packages}|
ditto above, for last 10% of commits
Jaccard - {u committed packages} ^ {U committed packages}
	(those last three are specific to the user as well)
time since last adoption
time between adoptions within last 10% of adoptions of this package
time between adoptions within last 10% of commits
time between commits within last 10% of commits of this package
time between commits within last 10% of all commits
current rank
	# of adoptions
	# of uses
	# of repos
	# of users using
	# of additions - # deletions (true additions/deletions only)

start generating user features - use Tim's code as a starting point
commit to my repo for now

use Python 3.6 - python3

also think about StackOverflow features
	will highly correlate SO features with negative case (not adopted)
	ie, lots of StackOverflow will mean negative adoption
may need to bias towards more recent data
	
	
6.14 notes	
	
extra user feature - user already adopted library	
one output file per month - dump all events, one per user-library pair
remove try/except around search call, Tim pushed new code

model will run through 2017 data, but doesn't know if adoption or not
	if predict adoption, have to treat it that way for remainder of simulation
is it actually rolling predictions?

Tim gone until July 6
generate data, ping Tim on slack (socsim) when done
weekly updates at least
yes, call in to meetings (sad)


6.25

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" 
time python3 classify.py 2>&1 | tee "predict_tests/test_$(date +"%F_%T").log"

loss types, 50 iterations, MinMax normaliztion, replace nan with 0

hinge	0
log		0.0007851347814708191
modified_huber	0
squared_hinge	0.16489865504830462
perceptron		0.07186527941649684
squared_loss	0.02398096130787537
huber					0
epsilon_insensitive		0
squared_epsilon_insensitive		0.04932920401495459

look at squared_hinge more closely - repeated runs F-scores

0.16489865504830462
0.1582151986985824
0.17332562997200124
0.19066993388506173
0.2020573473140188
0.1711954084835441

well, that seems reasonably consistent at least - what other knobs can I twiddle?

play with penalty:

none		
0.16021178546916362
0.16785731642124704
0.20099981135634784
0.19352344127597876

l2			
0.1591433763937325
0.17771563569741095
0.1655946220438168
0.16212630094953767

l1			
0.171350861287398
0.16630630630630633
0.17953303019465588
0.14326489273547557

elasticnet	
0.1589681504935742
0.17582527176623114
0.14634368040721718
0.1824235603076346

6.26

new attack - updated code to do a whole bunch of configurations in sequence

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" 

6.27

ran a bunch, squared_hinge seems overall best, so let's stick with that for now
penalty less clear, keep them all for now
let's play with some other features, try to dial in the discrete ones before we play with the continuous ones

6.28

finally finished generating feature vectors!!

finished 29250000 commits
    353 adopts pyswip.call at: 2018-05-19 18:19:32
    147720 adopts os at: 2018-05-20 05:28:47
    85144 adopts dotenv at: 2018-05-21 14:02:40
    75696 adopts argparse at: 2018-05-21 16:32:55
    154575 adopts __future__.print_function at: 2018-05-22 06:29:11
    18648 adopts scipy.sparse at: 2018-05-22 15:01:20
finished 29260000 commits
    154675 adopts ast at: 2018-05-27 05:19:06
    150787 adopts six.moves.configparser at: 2018-05-27 14:08:09
    150787 adopts multiprocessing at: 2018-05-27 14:08:09
    150787 adopts shlex at: 2018-05-27 14:08:09
    141702 adopts songs at: 2018-05-29 18:24:07
    141602 adopts tempfile at: 2018-05-30 05:07:14
    149031 adopts lib.slackClient.SlackClient at: 2018-05-31 00:40:31
saved events for 5-2018
saved events for 6-2018
EOF

real    10568m57.369s
user    10519m32.780s
sys     32m7.914s

so... 7 days and 8 hours to process ~29,260,000 commits

looks like everything through May 2018 is good, but I wouldn't trust June - file too small

6.29

hmmm... need to check when I cloned the first of the repos, since we can really only trust the commit data up to that point - anything after may not be complete for all repos

run some more tests with StandardScaler instead of MinMax

7.3

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" &

StandardScaler does poorly, stick with MinMax

run some new tests, try -1 to replace nan instead of 0

also, with value=0, see what forcing some classifications does based on implied non-adoptions

7.4

-1 for nan is pretty crappy, probably compresses the stuff we do care about too much to extract useful information

and... no labels changed by the manual check, we'll back that out and move on with our lives

ran new utility to add user, repo, package, and time to feature vectors - at the front, in that order

todo: update feature vector notes, update classify.py to use/handle new files?

7.5

did those couple todo items

I know I was cloning over spring break in March, so probably best to only trust the January/February stuff from 2018

for now, let's see if our classifier improves by only training on the last year or so, instead of allll the way back to 1993
	train 2014, predict Jan 2015
	
well, that ran faster at least - results any better?
	yep, not bad
	
try training on just the previous month?
	pretty bad actually
	
try 6 months of training data?
	not as good as a whole year
	
7.6

go through, feature ablation test - try different combinations of features
	especially those binary ones that are directly implying the labels
	remove StackOverflow, see what happens
see what features it likes, and which ones it doesn't - look at coefficients for clues

maybe reduce data to those where binary features are false - since those are really the events/libraries we're interested in

but first, update code to dump results to csv, because we've had enough copy-pasting

declare var and use in next command:
	STR="test.txt" && python3 classify.py > $STR
	
new test command - pipes output and generates results csv
	STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &

tests running:
	standard run, all features		4:37
	remove binary features 16 and 17	4:16, 18:37
	remove binary features and SO features		4:16, 18:33
	remove SO features		4:33

7.7

all those runs finished - files dated 7.6
	all using 2014 to predict Jan 2015 (no data range columns, forgot them)
	
do those 4 again, 5 times each, for the following training setups
	6 months of data
	1 month of data
	two years of data
	all the training data

7.8

uh oh, some of them died and didn't finish - let's dig out the failures (ugh)

well, crap... can't tell which ones died, because output files are empty

new plan - pull all the data we have, find the holes in that

okay, compiled all the data, should be good now
use num_features to determine which features we have
	33		standard run, all features		4:37
	31		remove binary features 16 and 17	4:16, 18:37
	27		remove binary features and SO features		4:16, 18:33
	29		remove SO features		4:33
	
have 160 entries for one month, six months, and one year of data - 20 per test number, 40 (5 runs) per feature removal
	only 88 for two years
	only 32 for 1993-2014
	
runs to fill holes:
	two years:
		3 binary	DONE
		1 SO		DONE
		5 none		DONE
	1993-2014:
		4 binary	RUNNING (first)
		4 binary+SO	RUNNING (second)
		3 none		DONE
		5 SO		RUNNING (third)

STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &		

loop form:

for i in {1..5}; do STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log"; done &

7.9

runs all done, commit those up

let's tweak the results compiler to remove leading spaced (ugh) and add some more discrete labels for training/testing period and features removed

oops - still some holes, made a booboo in my bash script
	1993-2014:
		3 binary		RUNNING
		3 binary+SO		RUNNING (later)

STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &

made some plots anyway, and (no surprise) removing the binary (seen/used before) features kills the classifier

but is this really cheating? I suppose yes, but the real problem is the data that we don't have

what we have: user-package events triggered by commits, one per committed package
what we're currently doing: classifying these events as adoptions or not

this is essentially trivial - if they've used it before, not an adoption, and if they haven't, adoption
no prediction going on at all

what we want to have: user-package events, triggered by commits, one per seen package (committed or not)
then we do: classify *these* events as adoptions or not

here's where the prediction comes in - given that a user has seen a set of packages, which ones will they adopt vs not? we're interested in the stuff that hasn't/doesn't happens

Tim meeting:
	yep, get ALL the data - negative samples
	both top-level and sub-packages - levels of granularity

	roll a dice - based on user's commit count and package commit count
		maybe incorporate StackOverflow popularity?
		
	makes the slides
	
let's dissect this code - adopt.py

looping libraries added in each commit - added_libs
updated_libs = set of libraries committed to repo since user's last commit to this repo
	we assume they "saw" these libraries, at the very least when pulling before they pushed
define adoption as a library in added_libs and updated_libs that is not in the user's quiver
	ie, user just committed this library for the first time, AND library recently added/updated in repo
what about adoptions from a different repo? or an exogenous source? this seems wrong...

hmm... get the negatives as we discussed, or redefine adoption to include any first usage?
	maybe it's just a terminology difference - intra-repo adoption?

7.10

let's just get the negative samples as discussed, because it's Tuesday and I don't care

required data from User for User-Package features:
	adopted_libs
	quiver
	seen_libs_freq
	lib_view_freq -> spec_lib, total_lib - based on last_commits
	
crap, need to regen everything - can't match up with old, because sets are sad

new features?
	# of packages in this commit		DONE
	# of updated packages between previous commit and this one		DONE
	break down percentages		DONE
	
	repo features? nope, not this time - because screw that
		number of contributing users (all time or last 10%)
		number of contributing users responsible for updated packages
		
running... I think (hope!)

7.11

still running, up to 11-2002 - hooray!

Tim will send me his old slides, aim for about 8 slides for 10 minutes of talk

keep babysitting the data gen, try to get a time estimate

write some new classifier/prediction(?) code

SLIDES

let's do some cursory analysis, because data gen is slow

first run, just basic counts:

year , commit_count , import_commit_count , num_users ,
1993 , 6936 , 1298 , 13
1994 , 13958 , 2136 , 18
1995 , 30126 , 3480 , 16
1996 , 37284 , 3939 , 27
1997 , 53766 , 4373 , 32
1998 , 73004 , 6281 , 59
1999 , 62020 , 5986 , 119
2000 , 120050 , 13277 , 252
2001 , 181902 , 20297 , 428
2002 , 187556 , 21872 , 620
2003 , 208286 , 30122 , 894
2004 , 234022 , 26965 , 1146
2005 , 307315 , 49380 , 1586
2006 , 521339 , 86422 , 2414
2007 , 787204 , 114632 , 3720
2008 , 1012142 , 150164 , 6401
2009 , 1262158 , 207409 , 10745
2010 , 1848185 , 286452 , 15005
2011 , 2459750 , 360666 , 20513
2012 , 3160030 , 458908 , 28743
2013 , 3590287 , 549301 , 38643
2014 , 3340657 , 516844 , 47132
2015 , 3328950 , 501049 , 55210
2016 , 2901661 , 465892 , 59458
2017 , 2986646 , 412787 , 59737

ALL , 28715234 , 4299932 , 160145

try some harder - can we identify and count the adoptions quickly?

plot: adoption events and import commits per week?
even just adoption events per year

TODO:
	why adoption count not working?
	classification tests, even if not all data - focus on perturbation
	SLIDES
	
7.12

circle back to adoption count later, focus on some classification tests - even if we don't have all the data

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/new_results_$STR" > "predict_tests/new_test_$STR.log" &

combos
	UPLS	all features	DONE
	UPL 	git history		DONE
	U		user only		RUNNING
	P		pair only		RUNNING
	L		library only	RUNNING
	UP		user and pair	RUNNING
	UL		user and library	RUNNING
	PL		pair and library	RUNNING
	ULS		user, lib, SO		RUNNING
	
STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/newest_results_$STR" > "predict_tests/newest_test_$STR.log" &

all done, plot all the things!

UPLS, 2 years = blue

hmmm... SO didn't exist then - why do the 0s help? must investigate further

also fixed the commit feature errors - Tim says not cheating, just need to break them out

run C alone, since it seems important
other combos running:
	UPC
	ULC
	ULSC
can do more later if we care

95% confidence intervals
note 5 tests

email Tim list of features - DONE
keep updated on data gen progress


7.23

back at it

data gen up to March 2012 - slow going, but going

checking some counts - flagged adoptions vs first usages

empty_events contains commit data for commits with no updated libs and no added libs (shouldn't need them, but keeping just in case...)

hmm... seems okay I guess? onward

TODO possibilities:
	more classify tests/results
	prediction (instead of classification)
	start writing paper
	
bigger/harder TODO:	
	* cost of learning/adoption - "I hypothesize that after an adoption event, users have a burst of commits (or maybe pushes), as they learn how to use the new library"
	* adoption as it correlates to code persistence - is the code associated with a new adoption likely to stick around or be deleted? (especially in a big project) - how people make errors, "cost of innovation"
	* library/package changeover
	* impact of user type (influencer vs follower) on adoption types
	
GitHub launched April 2008
StackOverflow launched September 2008

with that in mind, let's try some new tests - with the negative samples, starting 2008 or later

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &
	
running a single test, train 2011, classify Jan 2012

while that's running, let's think: what has to change for prediction vs classification?

classification: train on all included features, classify events using same features
	this requires commit data for the events being classified - some features based on timestamps, commit features based on specific commit

hmm... back up - for the classifier, are we really interested in instances where the user has already used a package?
	these obviously can't be adoptions, by any definition of the terminology
	what if we filter the classification to include only potential adoptions - actually adopted or not?
let's see how much that might cut down the dataset
	idx 22: user already used this package (binary)
	
consider Jan 2009:
	LABELS:
	1161454 user-package events
	2397 adoption events

	DATA:
	1161454 user-package events with 44 features
	32343 first-time usages
	70223 repeat usages
	986437 potential adoptions (including actual adoptions and all first-time uses)
	954094 missed adoptions
	104794 updated and not committed after previous usage
	102566 addition events
	1058888 non-additions

so of 1,161,454 events:
	102,566 addition events  = 8.83%
		32,343 first usages = 2.78%
			2,397 "true" adoptions = 0.206%
			29,946 unflagged adoptions = 2.58%
		70,223 repeat usages = 6.05%
	1,058,888 non-additions = 91.17%
		954,094 "missed" adoptions (never used, still not committed) = 82.15%
		104,794 updated and not committed after previous usage = 9.02%
	
so... roughly 15% of events are for a package the user has already committed
	or ~175K/1.2 million events
might make a difference? 
and are they really relevant for training? 
	user has already used package, not an adoption or non-adoption, so probably not a useful example

wrote some code to filter out the repeat uses - both actual additions and update non-additions
	so, should only have data for packages that *could* be adopted (ie, user not used before)
	
STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &
	
currently running:
	long test started 13:42 (taking forever)
	one month training:
		5 filtered (started first, 16:24)
		5 not (started after, 16:29)

7.24

data gen up to June 2012 - it's trying

some of the classify runs died - why? and which ones?
	ValueError: Found input variables with inconsistent numbers of samples: [3024443, 5040771]
	so, mismatch in size between labels and features somewhere - let's dig it out
	
found the error - stupid copy-paste fail, wasn't actually filtering the testing data, only the labels
	should be fixed now
	
oh, and we should add a filter flag to the results csv as well

tests running again, let's update the data we already have to include the filter column - DONE

tests finished, seemed to work fine - let's look at results, see if we can cut down on the number of combos
	for 7/8 tests, filtering results in a small improvement

more command line args, to allow for script-driven multiple runs? ponder

* cost of learning/adoption - "I hypothesize that after an adoption event, users have a burst of commits (or maybe pushes), as they learn how to use the new library" - YES, Tim likes this

sample down the negative cases

nah, no more command-line args - just another loop

running:
	1 year training, unfiltered x4
	1 year training, filtered x5
	6 months, filtered x5
	6 months, unfiltered x5
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5
let those run for now
	
TODO:
	look at results, try to narrow down the configurations to the best/most promising
	determine for sure if filtered data is the way to go
	try to tackle prediction vs classification
	read meme-tracker paper?
	downsampling?
	
7.25

data gen on August 2012

looks like some of the runs died - memory error? let's combine the data and see what holes we have to fill (hopefully not the long ones)

bleh - looks like one finished, one is still running, and the rest died pretty early
	guess we need to run them in smaller batches
	
6 months training, unfiltered x5 is what finished
1 year training, unfiltered is still running (midway through repeat #4)

one strange error though:
	testing_events_raw = testing_events_raw[rows[0]]
	IndexError: index 5040771 is out of bounds for axis 0 with size 5040771

runs:
	1 year training, unfiltered x4 - DONE
	1 year training, filtered x5 - RUNNING
	1 month, unfiltered x5 - DONE
	1 month, filtered x5 - DONE
	6 months, filtered x5 - RUNNING
	6 months, unfiltered x5	- DONE
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &

tried 6 months, filtered x5, died with the same IndexError - what's with that?
	ugh, found the error - copy-paste again
	also, don't think I can trust any of the existing filtered results, so better kill all of those
fixed bug, try that again

let's think about downsampling the negative cases

Jan 2009 breakdown of 1,161,454 events:
	102,566 addition events  = 8.83%
		32,343 first usages = 2.78%
			2,397 "true" adoptions = 0.206%
			29,946 unflagged adoptions = 2.58%
		70,223 repeat usages = 6.05%
	1,058,888 non-additions = 91.17%
		954,094 "missed" adoptions (never used, still not committed) = 82.15%
		104,794 updated and not committed after previous usage = 9.02%

filtering removes the 6.05% repeat usages and the 9.02% updated, but not added, previously used packages

this leaves a few categories:
	positive
		0.2% "true" adoptions
		2.5% unflagged first usages (kind of a negative?)
	negative
		82% missed/skipped adoptions (non-additions)
		
hella unbalanced, downsampling is probably a good idea - and might speed things up

(aside idea: can we weight longer training histories toward more recent stuff? weighting could be accomplished through biased sampling)

TODO:
	look at results so far, try to narrow down the configurations to the best/most promising
	determine for sure if filtered data is the way to go
	try to tackle prediction vs classification
	read meme-tracker paper?
	downsampling - finish and test
	
7.26

data gen update - October 2012
	seems to be slowing down?

runs:
	1 year training, unfiltered x4 - DONE
	1 year training, filtered x5 - DONE
	1 month, unfiltered x5 - DONE
	1 month, filtered x5 - DONE
	6 months, filtered x5 - DONE
	6 months, unfiltered x5	- DONE
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5
	
one run finished, other should be getting close

once those are done, need to narrow down the classifier configurations, because this is taking FOREVER
	hopefully also narrow to filtered or not before doing ablation test
	
added downsample column to output files (both existing and future)
	updated combine program accordingly
	
huzzah! data gen finished October 2012!

still waiting on filtered 1 year data (3 tests left!) - but need to make slides

filtered better than not? yes, in all cases (configurations and training periods)

yay! run finished - only took a day...
	updated plot for slides

adoption chains - do collaborators adopt the same library in sequence?

7.27

data gen to Jan 2013 - maybe faster when I don't have other stuff running? doubt it though

reduced classifier configurations by half, let's run a few tests (just 6 month training, 3x) with 4:1 downsample ratio
	3X - DONE (and quite fast too!)
	2X - DONE

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &

go ahead and do 1 year also - DONE
and 1 month - DONE

try a 1:1 ratio - all DONE
	1 year
	1 month
	6 months
	
7.30

data gen up to July 2013

runs finished, run same training periods with a 2:1 downsample ratio - DONE
	all done, make a plot (plots?)
	
meeting notes:
	write intro and abstract last
	maybe start with library adoption definition
	
	more experiments about what happens around library adoption
		what else does a user do with a newly adopted library?
	given an adoption (and a commit that wasn't an adoption), pull user's commits afterwards containing same libraray
		or include all of their commits, because refactor?
	compare to average burstiness over all (per individual user)
		burstiness - see paper? (skim only, very math)
		burstiness of everyday, of all users - typical activity, baseline
			for different strata of users - differernt burstiness
		burstiness score for all users - has some distribution
		burstiness of sequences immediately following an adoption - 12 hours? X hours?
			for all users, for all their adoptions
			
	burstiness of every user, all time, for all commits
	
	maybe instead of burstiness, just # of commits in a time window
		define a session - 1 hour, 2 hours, 3... etc
		distribution of commits within a session - time between activity
		
	session - continuous activity without a gap of x hours
		plot gap time on x-axis (discrete), # of commits meeting criteria on y-axis (see session plot layout image)
		pick from there
	average commits per session vs average commits per session containing adoption
		for a particular user
	single commit is a 0-hour session	
	
	for an individual, if delay between activity is more than X hours, started a new session
	
	see reddit paper for more details on defining session length - find delay, then using delay get avg session length
	
	given a session length, where do adoptions fall?
	given an adoption happens, is there more activity within a session than normal?
	
	
GitHub started April 2008, StackOverflow September 2008
		so probably start somewhere around there
	
let's make this plot!

first, does commit_analysis.py find all the adoptions?

year , commit_count , import_commit_count , num_users , intra-repo_adopts
1990 , 2523 , 264 , 6 , 0
1991 , 10680 , 1032 , 4 , 0
1992 , 15064 , 3080 , 12 , 20
1993 , 6936 , 1298 , 13 , 8
1994 , 13958 , 2136 , 18 , 0
1995 , 30126 , 3480 , 16 , 3
1996 , 37284 , 3939 , 27 , 13
1997 , 53766 , 4373 , 32 , 14
1998 , 73004 , 6281 , 59 , 43
1999 , 62020 , 5986 , 119 , 59
2000 , 120050 , 13277 , 252 , 499
2001 , 181902 , 20297 , 428 , 637
2002 , 187556 , 21872 , 620 , 653
2003 , 208286 , 30122 , 894 , 709
2004 , 234022 , 26965 , 1146 , 783
2005 , 307315 , 49380 , 1586 , 4859
2006 , 521339 , 86422 , 2414 , 12662
2007 , 787204 , 114632 , 3720 , 11185
2008 , 1012142 , 150164 , 6401 , 19368
2009 , 1262158 , 207409 , 10745 , 30817
2010 , 1848185 , 286452 , 15005 , 38029
2011 , 2459750 , 360666 , 20513 , 55263
2012 , 3160030 , 458908 , 28743 , 80796
2013 , 3590287 , 549301 , 38643 , 82672
2014 , 3340657 , 516844 , 47132 , 82210
2015 , 3328950 , 501049 , 55210 , 63233
2016 , 2901661 , 465892 , 59458 , 44994
2017 , 2986646 , 412787 , 59737 , 27569


2008
386 + 1068 + 2123 + 1026 + 1177 + 2212 + 1783 + 1467 + 923 + 2964 + 2950 + 1289 = 19368 adoption events

hooray! looks correct

how long does commit_analysis.py take for 1993 through 2018? a while - still running

to just stream with minimal processing? ~40 minutes

seems like the all_commits_by_year folder is only 4.2 GB, so let's go ahead and create new files - commits by user!
	chunk dictionary to 1000 users each (by user id) and pickle each one separately

chunking code seems to work!
	have to decide if want adoption labels on these - waiting on timed adoption count run to decide
	also waiting on a min/max user_id run, but I don't think it matters
	
7.31

data gen progress: August 28, 2013

results of min/max user id:
	min -5
	max 170412
hmm... negative 5? was that my sentinel? need to look into that

adoption-identifying run of commit_analysis.py took 286 minutes ~= 5 hours

instead of identifying the adoptions on the fly, could we pull the labels from the precomputed stuff?
	might be worth a try

trying - but have to keep track of empty_events too - ones where no libraries from added or updated
	actually, don't need any info from empty_events - those are commits with no imports, where no libs were updated
only care about commits where they added - and could have adopted

something is failing, commit user mismatch - why?

FAIL
2547 {'add_libs': [], 'del_libs': [], 'user': 16996, 'repo': 'ppython__nascheme', 'time': 672262048}
[2547, 16995, 'python-ast__banga', 662753474, 1, 0, 0, 49, 0, 0, 34, 662753475, 957341.5607223321, 2, 2, 0.0, 0.10377358490566038, 0.0, 0.09090909090909091, 'poly', 1, 0, 0, 0, 0, None, 0, 0, None, 0, 0, 0, 0, 0, None, None, None, None, None, None, 0, 0, 0, 0] 0

features are first commit in 1-1991
but date of commit says 4-1991?

ugh... bad counter reset, everything is fine now

fixed the code, started it up for 1990-2012 (as far as we can go for now)
	see how long it takes! (and if we overflow the memory...)
	
so... theoretically we have a list of commits by user (and sorted for each user)

now to create the delay/session plot:
	plot gap time on x-axis (discrete), # of commits meeting criteria on y-axis (see session plot layout image)
so for each user:
	step through commits in time-order
	compute delay between current commit and previous
	increment counter of corresponding delay bin
	plot!
bins:
	exactly 0 (simultaneous commits)
	<= 1
	1-2 hours
	2-3 hours
	....
	etc
do all hour-wide bins for now and save the data, can always combine later

user commit files are done! for 1990-2011
	~46 minutes to run
	need to rerun once all data gen is done
spot-check on the files looks good!	
yep, commit counts match up - everything is accounted for!

delay program running - plot later
	finished, only takes ~2.5 minutes
	
8.1

data gen up to October 2013 - just keep running, just keep running...

let's get this plot made - even if Excel doesn't want to

whip up a quick plot - ugh, no clear break point

try again with half-hour bins?
	maybe helpful?
	
made a first-pass at session analysis - identifying sessions given a maximum inactivity period, need to decide what data to dump for plotting

8.2

data gen up to 12-24-2013 - woohoo, almost finished 2013!




