6.11 notes

how to train a model off this data? regression?

every commit is an instance, what are our feature vectors?
	sometimes an adoption happens, sometimes not
	
StackOverflow - 16 million questions, 25 million answers
actually, ~40 million posts - according to Tim
	
User Features - specific to one particular user
*****
# packages commited - half-life?
# packages implicitly seen
time since last commit
time since last adoption
intra-commit duration for last 10% of commits
# repositories commited to
# repositories commited to in last 10% of commits
% commits with adoptions
% commits with imports
% commits with adoptions within last 10% of commits
% commits with imports within last 10% of commits
# implicitly seen package i
# implicitly seen package i within last 10% of commits
# implicitly seen package i / total # implicit packages seen
# implicitly seen package i within last 10% commits / total # implicit packages seen within last 10% commits
{packages implicitly seen}

Package Features - specific to one particular package 
*****
# of addition commits
# users who have committed
# users who have adopted (# adoptions)
# repos containing package
|{u committed packages} ^ {U_(x in U ^ x adopted i) all committed packages}|
ditto above, for last 10% of commits
Jaccard - {u committed packages} ^ {U committed packages}
	(those last three are specific to the user as well)
time since last adoption
time between adoptions within last 10% of adoptions of this package
time between adoptions within last 10% of commits
time between commits within last 10% of commits of this package
time between commits within last 10% of all commits
current rank
	# of adoptions
	# of uses
	# of repos
	# of users using
	# of additions - # deletions (true additions/deletions only)

start generating user features - use Tim's code as a starting point
commit to my repo for now

use Python 3.6 - python3

also think about StackOverflow features
	will highly correlate SO features with negative case (not adopted)
	ie, lots of StackOverflow will mean negative adoption
may need to bias towards more recent data
	
	
6.14 notes	
	
extra user feature - user already adopted library	
one output file per month - dump all events, one per user-library pair
remove try/except around search call, Tim pushed new code

model will run through 2017 data, but doesn't know if adoption or not
	if predict adoption, have to treat it that way for remainder of simulation
is it actually rolling predictions?

Tim gone until July 6
generate data, ping Tim on slack (socsim) when done
weekly updates at least
yes, call in to meetings (sad)


6.25

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" 
time python3 classify.py 2>&1 | tee "predict_tests/test_$(date +"%F_%T").log"

loss types, 50 iterations, MinMax normaliztion, replace nan with 0

hinge	0
log		0.0007851347814708191
modified_huber	0
squared_hinge	0.16489865504830462
perceptron		0.07186527941649684
squared_loss	0.02398096130787537
huber					0
epsilon_insensitive		0
squared_epsilon_insensitive		0.04932920401495459

look at squared_hinge more closely - repeated runs F-scores

0.16489865504830462
0.1582151986985824
0.17332562997200124
0.19066993388506173
0.2020573473140188
0.1711954084835441

well, that seems reasonably consistent at least - what other knobs can I twiddle?

play with penalty:

none		
0.16021178546916362
0.16785731642124704
0.20099981135634784
0.19352344127597876

l2			
0.1591433763937325
0.17771563569741095
0.1655946220438168
0.16212630094953767

l1			
0.171350861287398
0.16630630630630633
0.17953303019465588
0.14326489273547557

elasticnet	
0.1589681504935742
0.17582527176623114
0.14634368040721718
0.1824235603076346

6.26

new attack - updated code to do a whole bunch of configurations in sequence

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" 

6.27

ran a bunch, squared_hinge seems overall best, so let's stick with that for now
penalty less clear, keep them all for now
let's play with some other features, try to dial in the discrete ones before we play with the continuous ones

6.28

finally finished generating feature vectors!!

finished 29250000 commits
    353 adopts pyswip.call at: 2018-05-19 18:19:32
    147720 adopts os at: 2018-05-20 05:28:47
    85144 adopts dotenv at: 2018-05-21 14:02:40
    75696 adopts argparse at: 2018-05-21 16:32:55
    154575 adopts __future__.print_function at: 2018-05-22 06:29:11
    18648 adopts scipy.sparse at: 2018-05-22 15:01:20
finished 29260000 commits
    154675 adopts ast at: 2018-05-27 05:19:06
    150787 adopts six.moves.configparser at: 2018-05-27 14:08:09
    150787 adopts multiprocessing at: 2018-05-27 14:08:09
    150787 adopts shlex at: 2018-05-27 14:08:09
    141702 adopts songs at: 2018-05-29 18:24:07
    141602 adopts tempfile at: 2018-05-30 05:07:14
    149031 adopts lib.slackClient.SlackClient at: 2018-05-31 00:40:31
saved events for 5-2018
saved events for 6-2018
EOF

real    10568m57.369s
user    10519m32.780s
sys     32m7.914s

so... 7 days and 8 hours to process ~29,260,000 commits

looks like everything through May 2018 is good, but I wouldn't trust June - file too small

6.29

hmmm... need to check when I cloned the first of the repos, since we can really only trust the commit data up to that point - anything after may not be complete for all repos

run some more tests with StandardScaler instead of MinMax

7.3

time python3 classify.py > "predict_tests/test_$(date +"%F_%T").log" &

StandardScaler does poorly, stick with MinMax

run some new tests, try -1 to replace nan instead of 0

also, with value=0, see what forcing some classifications does based on implied non-adoptions

7.4

-1 for nan is pretty crappy, probably compresses the stuff we do care about too much to extract useful information

and... no labels changed by the manual check, we'll back that out and move on with our lives

ran new utility to add user, repo, package, and time to feature vectors - at the front, in that order

todo: update feature vector notes, update classify.py to use/handle new files?

7.5

did those couple todo items

I know I was cloning over spring break in March, so probably best to only trust the January/February stuff from 2018

for now, let's see if our classifier improves by only training on the last year or so, instead of allll the way back to 1993
	train 2014, predict Jan 2015
	
well, that ran faster at least - results any better?
	yep, not bad
	
try training on just the previous month?
	pretty bad actually
	
try 6 months of training data?
	not as good as a whole year
	
7.6

go through, feature ablation test - try different combinations of features
	especially those binary ones that are directly implying the labels
	remove StackOverflow, see what happens
see what features it likes, and which ones it doesn't - look at coefficients for clues

maybe reduce data to those where binary features are false - since those are really the events/libraries we're interested in

but first, update code to dump results to csv, because we've had enough copy-pasting

declare var and use in next command:
	STR="test.txt" && python3 classify.py > $STR
	
new test command - pipes output and generates results csv
	STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &

tests running:
	standard run, all features		4:37
	remove binary features 16 and 17	4:16, 18:37
	remove binary features and SO features		4:16, 18:33
	remove SO features		4:33

7.7

all those runs finished - files dated 7.6
	all using 2014 to predict Jan 2015 (no data range columns, forgot them)
	
do those 4 again, 5 times each, for the following training setups
	6 months of data
	1 month of data
	two years of data
	all the training data

7.8

uh oh, some of them died and didn't finish - let's dig out the failures (ugh)

well, crap... can't tell which ones died, because output files are empty

new plan - pull all the data we have, find the holes in that

okay, compiled all the data, should be good now
use num_features to determine which features we have
	33		standard run, all features		4:37
	31		remove binary features 16 and 17	4:16, 18:37
	27		remove binary features and SO features		4:16, 18:33
	29		remove SO features		4:33
	
have 160 entries for one month, six months, and one year of data - 20 per test number, 40 (5 runs) per feature removal
	only 88 for two years
	only 32 for 1993-2014
	
runs to fill holes:
	two years:
		3 binary	DONE
		1 SO		DONE
		5 none		DONE
	1993-2014:
		4 binary	RUNNING (first)
		4 binary+SO	RUNNING (second)
		3 none		DONE
		5 SO		RUNNING (third)

STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &		

loop form:

for i in {1..5}; do STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log"; done &

7.9

runs all done, commit those up

let's tweak the results compiler to remove leading spaced (ugh) and add some more discrete labels for training/testing period and features removed

oops - still some holes, made a booboo in my bash script
	1993-2014:
		3 binary		RUNNING
		3 binary+SO		RUNNING (later)

STR="$(date +"%F_%T")" && time python3 classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &

made some plots anyway, and (no surprise) removing the binary (seen/used before) features kills the classifier

but is this really cheating? I suppose yes, but the real problem is the data that we don't have

what we have: user-package events triggered by commits, one per committed package
what we're currently doing: classifying these events as adoptions or not

this is essentially trivial - if they've used it before, not an adoption, and if they haven't, adoption
no prediction going on at all

what we want to have: user-package events, triggered by commits, one per seen package (committed or not)
then we do: classify *these* events as adoptions or not

here's where the prediction comes in - given that a user has seen a set of packages, which ones will they adopt vs not? we're interested in the stuff that hasn't/doesn't happens

Tim meeting:
	yep, get ALL the data - negative samples
	both top-level and sub-packages - levels of granularity

	roll a dice - based on user's commit count and package commit count
		maybe incorporate StackOverflow popularity?
		
	makes the slides
	
let's dissect this code - adopt.py

looping libraries added in each commit - added_libs
updated_libs = set of libraries committed to repo since user's last commit to this repo
	we assume they "saw" these libraries, at the very least when pulling before they pushed
define adoption as a library in added_libs and updated_libs that is not in the user's quiver
	ie, user just committed this library for the first time, AND library recently added/updated in repo
what about adoptions from a different repo? or an exogenous source? this seems wrong...

hmm... get the negatives as we discussed, or redefine adoption to include any first usage?
	maybe it's just a terminology difference - intra-repo adoption?

7.10

let's just get the negative samples as discussed, because it's Tuesday and I don't care

required data from User for User-Package features:
	adopted_libs
	quiver
	seen_libs_freq
	lib_view_freq -> spec_lib, total_lib - based on last_commits
	
crap, need to regen everything - can't match up with old, because sets are sad

new features?
	# of packages in this commit		DONE
	# of updated packages between previous commit and this one		DONE
	break down percentages		DONE
	
	repo features? nope, not this time - because screw that
		number of contributing users (all time or last 10%)
		number of contributing users responsible for updated packages
		
running... I think (hope!)

7.11

still running, up to 11-2002 - hooray!

Tim will send me his old slides, aim for about 8 slides for 10 minutes of talk

keep babysitting the data gen, try to get a time estimate

write some new classifier/prediction(?) code

SLIDES

let's do some cursory analysis, because data gen is slow

first run, just basic counts:

year , commit_count , import_commit_count , num_users ,
1993 , 6936 , 1298 , 13
1994 , 13958 , 2136 , 18
1995 , 30126 , 3480 , 16
1996 , 37284 , 3939 , 27
1997 , 53766 , 4373 , 32
1998 , 73004 , 6281 , 59
1999 , 62020 , 5986 , 119
2000 , 120050 , 13277 , 252
2001 , 181902 , 20297 , 428
2002 , 187556 , 21872 , 620
2003 , 208286 , 30122 , 894
2004 , 234022 , 26965 , 1146
2005 , 307315 , 49380 , 1586
2006 , 521339 , 86422 , 2414
2007 , 787204 , 114632 , 3720
2008 , 1012142 , 150164 , 6401
2009 , 1262158 , 207409 , 10745
2010 , 1848185 , 286452 , 15005
2011 , 2459750 , 360666 , 20513
2012 , 3160030 , 458908 , 28743
2013 , 3590287 , 549301 , 38643
2014 , 3340657 , 516844 , 47132
2015 , 3328950 , 501049 , 55210
2016 , 2901661 , 465892 , 59458
2017 , 2986646 , 412787 , 59737

ALL , 28715234 , 4299932 , 160145

try some harder - can we identify and count the adoptions quickly?

plot: adoption events and import commits per week?
even just adoption events per year

TODO:
	why adoption count not working?
	classification tests, even if not all data - focus on perturbation
	SLIDES
	
7.12

circle back to adoption count later, focus on some classification tests - even if we don't have all the data

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/new_results_$STR" > "predict_tests/new_test_$STR.log" &

combos
	UPLS	all features	DONE
	UPL 	git history		DONE
	U		user only		RUNNING
	P		pair only		RUNNING
	L		library only	RUNNING
	UP		user and pair	RUNNING
	UL		user and library	RUNNING
	PL		pair and library	RUNNING
	ULS		user, lib, SO		RUNNING
	
STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/newest_results_$STR" > "predict_tests/newest_test_$STR.log" &

all done, plot all the things!

UPLS, 2 years = blue

hmmm... SO didn't exist then - why do the 0s help? must investigate further

also fixed the commit feature errors - Tim says not cheating, just need to break them out

run C alone, since it seems important
other combos running:
	UPC
	ULC
	ULSC
can do more later if we care

95% confidence intervals
note 5 tests

email Tim list of features - DONE
keep updated on data gen progress


7.23

back at it

data gen up to March 2012 - slow going, but going

checking some counts - flagged adoptions vs first usages

empty_events contains commit data for commits with no updated libs and no added libs (shouldn't need them, but keeping just in case...)

hmm... seems okay I guess? onward

TODO possibilities:
	more classify tests/results
	prediction (instead of classification)
	start writing paper
	
bigger/harder TODO:	
	* cost of learning/adoption - "I hypothesize that after an adoption event, users have a burst of commits (or maybe pushes), as they learn how to use the new library"
	* adoption as it correlates to code persistence - is the code associated with a new adoption likely to stick around or be deleted? (especially in a big project) - how people make errors, "cost of innovation"
	* library/package changeover
	* impact of user type (influencer vs follower) on adoption types
	
GitHub launched April 2008
StackOverflow launched September 2008

with that in mind, let's try some new tests - with the negative samples, starting 2008 or later

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" > "predict_tests/test_$STR.log" &
	
running a single test, train 2011, classify Jan 2012

while that's running, let's think: what has to change for prediction vs classification?

classification: train on all included features, classify events using same features
	this requires commit data for the events being classified - some features based on timestamps, commit features based on specific commit

hmm... back up - for the classifier, are we really interested in instances where the user has already used a package?
	these obviously can't be adoptions, by any definition of the terminology
	what if we filter the classification to include only potential adoptions - actually adopted or not?
let's see how much that might cut down the dataset
	idx 22: user already used this package (binary)
	
consider Jan 2009:
	LABELS:
	1161454 user-package events
	2397 adoption events

	DATA:
	1161454 user-package events with 44 features
	32343 first-time usages
	70223 repeat usages
	986437 potential adoptions (including actual adoptions and all first-time uses)
	954094 missed adoptions
	104794 updated and not committed after previous usage
	102566 addition events
	1058888 non-additions

so of 1,161,454 events:
	102,566 addition events  = 8.83%
		32,343 first usages = 2.78%
			2,397 "true" adoptions = 0.206%
			29,946 unflagged adoptions = 2.58%
		70,223 repeat usages = 6.05%
	1,058,888 non-additions = 91.17%
		954,094 "missed" adoptions (never used, still not committed) = 82.15%
		104,794 updated and not committed after previous usage = 9.02%
	
so... roughly 15% of events are for a package the user has already committed
	or ~175K/1.2 million events
might make a difference? 
and are they really relevant for training? 
	user has already used package, not an adoption or non-adoption, so probably not a useful example

wrote some code to filter out the repeat uses - both actual additions and update non-additions
	so, should only have data for packages that *could* be adopted (ie, user not used before)
	
STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &
	
currently running:
	long test started 13:42 (taking forever)
	one month training:
		5 filtered (started first, 16:24)
		5 not (started after, 16:29)

7.24

data gen up to June 2012 - it's trying

some of the classify runs died - why? and which ones?
	ValueError: Found input variables with inconsistent numbers of samples: [3024443, 5040771]
	so, mismatch in size between labels and features somewhere - let's dig it out
	
found the error - stupid copy-paste fail, wasn't actually filtering the testing data, only the labels
	should be fixed now
	
oh, and we should add a filter flag to the results csv as well

tests running again, let's update the data we already have to include the filter column - DONE

tests finished, seemed to work fine - let's look at results, see if we can cut down on the number of combos
	for 7/8 tests, filtering results in a small improvement

more command line args, to allow for script-driven multiple runs? ponder

* cost of learning/adoption - "I hypothesize that after an adoption event, users have a burst of commits (or maybe pushes), as they learn how to use the new library" - YES, Tim likes this

sample down the negative cases

nah, no more command-line args - just another loop

running:
	1 year training, unfiltered x4
	1 year training, filtered x5
	6 months, filtered x5
	6 months, unfiltered x5
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5
let those run for now
	
TODO:
	look at results, try to narrow down the configurations to the best/most promising
	determine for sure if filtered data is the way to go
	try to tackle prediction vs classification
	read meme-tracker paper?
	downsampling?
	
7.25

data gen on August 2012

looks like some of the runs died - memory error? let's combine the data and see what holes we have to fill (hopefully not the long ones)

bleh - looks like one finished, one is still running, and the rest died pretty early
	guess we need to run them in smaller batches
	
6 months training, unfiltered x5 is what finished
1 year training, unfiltered is still running (midway through repeat #4)

one strange error though:
	testing_events_raw = testing_events_raw[rows[0]]
	IndexError: index 5040771 is out of bounds for axis 0 with size 5040771

runs:
	1 year training, unfiltered x4 - DONE
	1 year training, filtered x5 - RUNNING
	1 month, unfiltered x5 - DONE
	1 month, filtered x5 - DONE
	6 months, filtered x5 - RUNNING
	6 months, unfiltered x5	- DONE
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &

tried 6 months, filtered x5, died with the same IndexError - what's with that?
	ugh, found the error - copy-paste again
	also, don't think I can trust any of the existing filtered results, so better kill all of those
fixed bug, try that again

let's think about downsampling the negative cases

Jan 2009 breakdown of 1,161,454 events:
	102,566 addition events  = 8.83%
		32,343 first usages = 2.78%
			2,397 "true" adoptions = 0.206%
			29,946 unflagged adoptions = 2.58%
		70,223 repeat usages = 6.05%
	1,058,888 non-additions = 91.17%
		954,094 "missed" adoptions (never used, still not committed) = 82.15%
		104,794 updated and not committed after previous usage = 9.02%

filtering removes the 6.05% repeat usages and the 9.02% updated, but not added, previously used packages

this leaves a few categories:
	positive
		0.2% "true" adoptions
		2.5% unflagged first usages (kind of a negative?)
	negative
		82% missed/skipped adoptions (non-additions)
		
hella unbalanced, downsampling is probably a good idea - and might speed things up

(aside idea: can we weight longer training histories toward more recent stuff? weighting could be accomplished through biased sampling)

TODO:
	look at results so far, try to narrow down the configurations to the best/most promising
	determine for sure if filtered data is the way to go
	try to tackle prediction vs classification
	read meme-tracker paper?
	downsampling - finish and test
	
7.26

data gen update - October 2012
	seems to be slowing down?

runs:
	1 year training, unfiltered x4 - DONE
	1 year training, filtered x5 - DONE
	1 month, unfiltered x5 - DONE
	1 month, filtered x5 - DONE
	6 months, filtered x5 - DONE
	6 months, unfiltered x5	- DONE
	3 months, unfiltered x5
	3 months, filtered x5
	2 years, filtered x5
	2 years, unfiltered x5
	
one run finished, other should be getting close

once those are done, need to narrow down the classifier configurations, because this is taking FOREVER
	hopefully also narrow to filtered or not before doing ablation test
	
added downsample column to output files (both existing and future)
	updated combine program accordingly
	
huzzah! data gen finished October 2012!

still waiting on filtered 1 year data (3 tests left!) - but need to make slides

filtered better than not? yes, in all cases (configurations and training periods)

yay! run finished - only took a day...
	updated plot for slides

adoption chains - do collaborators adopt the same library in sequence?

7.27

data gen to Jan 2013 - maybe faster when I don't have other stuff running? doubt it though

reduced classifier configurations by half, let's run a few tests (just 6 month training, 3x) with 4:1 downsample ratio
	3X - DONE (and quite fast too!)
	2X - DONE

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &

go ahead and do 1 year also - DONE
and 1 month - DONE

try a 1:1 ratio - all DONE
	1 year
	1 month
	6 months
	
WOW - AUROC north of 0.9, hurray downsampling!
	
7.30

data gen up to July 2013

runs finished, run same training periods with a 2:1 downsample ratio - DONE
	all done, make a plot (plots?)
	
meeting notes:
	write intro and abstract last
	maybe start with library adoption definition
	
	more experiments about what happens around library adoption
		what else does a user do with a newly adopted library?
	given an adoption (and a commit that wasn't an adoption), pull user's commits afterwards containing same libraray
		or include all of their commits, because refactor?
	compare to average burstiness over all (per individual user)
		burstiness - see paper? (skim only, very math)
		burstiness of everyday, of all users - typical activity, baseline
			for different strata of users - differernt burstiness
		burstiness score for all users - has some distribution
		burstiness of sequences immediately following an adoption - 12 hours? X hours?
			for all users, for all their adoptions
			
	burstiness of every user, all time, for all commits
	
	maybe instead of burstiness, just # of commits in a time window
		define a session - 1 hour, 2 hours, 3... etc
		distribution of commits within a session - time between activity
		
	session - continuous activity without a gap of x hours
		plot gap time on x-axis (discrete), # of commits meeting criteria on y-axis (see session plot layout image)
		pick from there
	average commits per session vs average commits per session containing adoption
		for a particular user
	single commit is a 0-hour session	
	
	for an individual, if delay between activity is more than X hours, started a new session
	
	see reddit paper for more details on defining session length - find delay, then using delay get avg session length
	
	given a session length, where do adoptions fall?
	given an adoption happens, is there more activity within a session than normal?
	
	
GitHub started April 2008, StackOverflow September 2008
		so probably start somewhere around there
	
let's make this plot!

first, does commit_analysis.py find all the adoptions?

year , commit_count , import_commit_count , num_users , intra-repo_adopts
1990 , 2523 , 264 , 6 , 0
1991 , 10680 , 1032 , 4 , 0
1992 , 15064 , 3080 , 12 , 20
1993 , 6936 , 1298 , 13 , 8
1994 , 13958 , 2136 , 18 , 0
1995 , 30126 , 3480 , 16 , 3
1996 , 37284 , 3939 , 27 , 13
1997 , 53766 , 4373 , 32 , 14
1998 , 73004 , 6281 , 59 , 43
1999 , 62020 , 5986 , 119 , 59
2000 , 120050 , 13277 , 252 , 499
2001 , 181902 , 20297 , 428 , 637
2002 , 187556 , 21872 , 620 , 653
2003 , 208286 , 30122 , 894 , 709
2004 , 234022 , 26965 , 1146 , 783
2005 , 307315 , 49380 , 1586 , 4859
2006 , 521339 , 86422 , 2414 , 12662
2007 , 787204 , 114632 , 3720 , 11185
2008 , 1012142 , 150164 , 6401 , 19368
2009 , 1262158 , 207409 , 10745 , 30817
2010 , 1848185 , 286452 , 15005 , 38029
2011 , 2459750 , 360666 , 20513 , 55263
2012 , 3160030 , 458908 , 28743 , 80796
2013 , 3590287 , 549301 , 38643 , 82672
2014 , 3340657 , 516844 , 47132 , 82210
2015 , 3328950 , 501049 , 55210 , 63233
2016 , 2901661 , 465892 , 59458 , 44994
2017 , 2986646 , 412787 , 59737 , 27569


2008
386 + 1068 + 2123 + 1026 + 1177 + 2212 + 1783 + 1467 + 923 + 2964 + 2950 + 1289 = 19368 adoption events

hooray! looks correct

how long does commit_analysis.py take for 1993 through 2018? a while - still running

to just stream with minimal processing? ~40 minutes

seems like the all_commits_by_year folder is only 4.2 GB, so let's go ahead and create new files - commits by user!
	chunk dictionary to 1000 users each (by user id) and pickle each one separately

chunking code seems to work!
	have to decide if want adoption labels on these - waiting on timed adoption count run to decide
	also waiting on a min/max user_id run, but I don't think it matters
	
7.31

data gen progress: August 28, 2013

results of min/max user id:
	min -5
	max 170412
hmm... negative 5? was that my sentinel? need to look into that

adoption-identifying run of commit_analysis.py took 286 minutes ~= 5 hours

instead of identifying the adoptions on the fly, could we pull the labels from the precomputed stuff?
	might be worth a try

trying - but have to keep track of empty_events too - ones where no libraries from added or updated
	actually, don't need any info from empty_events - those are commits with no imports, where no libs were updated
only care about commits where they added - and could have adopted

something is failing, commit user mismatch - why?

FAIL
2547 {'add_libs': [], 'del_libs': [], 'user': 16996, 'repo': 'ppython__nascheme', 'time': 672262048}
[2547, 16995, 'python-ast__banga', 662753474, 1, 0, 0, 49, 0, 0, 34, 662753475, 957341.5607223321, 2, 2, 0.0, 0.10377358490566038, 0.0, 0.09090909090909091, 'poly', 1, 0, 0, 0, 0, None, 0, 0, None, 0, 0, 0, 0, 0, None, None, None, None, None, None, 0, 0, 0, 0] 0

features are first commit in 1-1991
but date of commit says 4-1991?

ugh... bad counter reset, everything is fine now

fixed the code, started it up for 1990-2012 (as far as we can go for now)
	see how long it takes! (and if we overflow the memory...)
	
so... theoretically we have a list of commits by user (and sorted for each user)

now to create the delay/session plot:
	plot gap time on x-axis (discrete), # of commits meeting criteria on y-axis (see session plot layout image)
so for each user:
	step through commits in time-order
	compute delay between current commit and previous
	increment counter of corresponding delay bin
	plot!
bins:
	exactly 0 (simultaneous commits)
	<= 1
	1-2 hours
	2-3 hours
	....
	etc
do all hour-wide bins for now and save the data, can always combine later

user commit files are done! for 1990-2011
	~46 minutes to run
	need to rerun once all data gen is done
spot-check on the files looks good!	
yep, commit counts match up - everything is accounted for!

delay program running - plot later
	finished, only takes ~2.5 minutes
	
8.1

data gen up to October 2013 - just keep running, just keep running...

let's get this plot made - even if Excel doesn't want to

whip up a quick plot - ugh, no clear break point

try again with half-hour bins?
	maybe helpful?
	
made a first-pass at session analysis - identifying sessions given a maximum inactivity period, need to decide what data to dump for plotting

8.2

data gen up to 12-24-2013 - woohoo, almost finished 2013!

added more to sessions code - track session length->freq and number of commits
	ran fine!
Processed 9435270 commits and 36341 users in 1034580 sessions
	~2 minutes runtime (not bad)

started code to identify adoptions within sessions, need to decide what data to track and output
mostly compare adoptive vs regular sessions:
	average commits per session
	session length
but also where adoptions fall within the session window

8.3

data gen up to 2-20-2014

clean up the repo directory - move some results files, etc

hmmm... my session adopt code isn't great - only tracking on a session basis, not a user basis
	more variables!

pgf plots, need error bars
http://pgfplots.sourceforge.net/

send email to Kristina - check sessions methodology

next up: look at where adoptions fall within a session, and think of some clear way to convey that

8.6

data gen up to 8-21-2014

refactor session logging to separate function before we add more variables

plot distribution of where adoptions fall
	got the data - both time and position
	and plotted
	
next - make cpm plot normalized about time of first adoption

cpm - commits per minute
	how best to average over many different user sessions?
	
first, how to compute cpm for one single user session?
	step through session over time, in 1 minute windows
	count the commits per window
	that is the cpm of this particular session
each discrete window will have cpm of 0 to pos whole number - because only one session is being considered

what about an average cpm over all of a user's sessions?

something like this?
https://stackoverflow.com/questions/11731735/how-to-calculate-continuous-smooth-event-rate-based-on-event-times
but considering all of a user's sessions simultaneously, then divide by # of sessions?
for each 1 minute window: sum(# of commits across all sessions) / # of sessions

how does this work for variable-length sessions?
	compute cpm as above for each session, then normalize, then average?
ponder...

8.7

data gen up to 10-27-14

Tim home Wed-Fri
	in DC on Monday
	
pmf discussion on Slack
	create an x-normalized PMF (normalize either side of adoption by time)
	and then stack/sum the weights together across sessions
	and then we can normalize y axis if we need to

	but also bin by 1%, then maybe Gaussian smoothing? ugh
	
	
User 51047 made 495 commits across 137 sessions
    21 libraries adopted in 5 commits across 5 sessions

8.8

data gen up to 12-28-14

just keep coding, just keep coding...

have discontinuous cpm for each session

TODO:
	normalize cpm - entire session for non-adopt, either side for adopt
	convert to 1% bins
	convert to pmf
	combine!
	gaussian smooth?
	
data gen: 1-12-15

8.9

data gen up to 3-12-15	(faster?)

let's do normalization and pmf in the same step - yes? sure
	nope, pmf needs to be for the entire session, not just one half - so need to do that first

okay, have the pmf for the entire session - now to normalize
	adoption sessions normalized on either side of adoption
	non-adopt normalized all at once
but need to be able to stack sessions of the same type together, and plot all sessions at once

so let's do this:
	normalize either side of the adoption to 0-100 (or -100 to 0)
	normalize regular sessions to -100 to 100
	
normalize to range (a, b) using X′ = a + [(X − Xmin)(b − a) / (Xmax − Xmin)] 

yay plot! it's so beautiful (and interesting)

from meeting:
	need to check normalization, see if magnitude is only due to that, or if actually true
	draw same figure for only import events (not all commits)
	learning curve? maybe worth formalizing
	
also - plot user's average session length vs frequency of that average
	ie, distribution of session lengths (already done?)

for other team members, from Python data:
	correlation between # of (changed) lines of code per commit per push per user vs team size
	keep in mind that larger code bases have more code, and are harder to change - how to measure complexity?
		total code size?
		number and size of edited files?
		how many individuals have touched a file?

see slack for more todo

8.10

data gen up to 5-9-2015

todo:
	skip pmf step for each session (stack and average commit counts)
	skip time normalization (stack by minute around adoption or session start)
	and all combinations thereof (boolean flags)
	
done - roughly
	
8.13
	
data gen up to 12-31-2015

let's gather some thoughts - what is the data saying?

	inter-commit delay plot led to selection of a maximum delay of 9 hours
	~94% of all sessions are then < 9 hours
	short sessions more common than long
		adoption sessions follow the same pattern, but there account for only ~1 percent of all sessions
	number of commits increases and session length increases, but not exponentially
		adoption sessions *appear* to have the same or more commits as regular sessions
	for normal users (short sessions), adoptions tend to appear earlier in the session (needs more investigation)
	
	PMF + time normalization suggests that sessions contain more commits after an adoption and fewer before (as compared to typical non-adopt sessions) - in this plot, each session has equal weight
	
	CPM + time normalization shows a similar different between pre- and post-adopt, but both sections have more commits than non-adopt sessions
	
	CPM + no time normalization: 
		in non-adopt sessions, commits occur way more frequently at the end of the session - unless that's just the long-session bots screwing it up?
		similar behavior in adoption sessions - but those sessions are even longer!
		
	perhaps, for this session analysis, we should throw out any sessions longer than ~24 hours?
		kind of a natural drop-off at 15/16 hour long sessions - which makes sense for humans - so maybe there	
		
TODO:
	average commit rate of different session types (finer grain than binning)
	binned version of average adopt commit # by number of commits in session (smooth out that curve)
		ditto for time - too zig-zaggy, can't tell what's actually going on
	length-limited session analysis
	
let's start with that last item, and only consider sessions 16 hours or shorter

made plots, look them over tomorrow

8.14

data gen up to 3-10-2016

changed session analysis plots to log scale y-axis

but some weirdness to check:
	limiting the length should make adopt and non-adopt plots cover the same range - but adopt seems longer?
	where is the big central peak in the normalized non-adopt plot coming from?
	
first - investigate the central peak

welp - it's a normalization artifact - disappears if normalize 0-100 instead of -100 to 0
	what if we normalize the adopt sessions to 0-100, with adopt at 50, for comparison?
	better
	
notes from Tim:
	for adoption sessions, stack/avg about first adoption, run 4-8 hours to either side
	for non-adopt sessions, two plots:
		time normalized stack (exposition on general behavior)
		regular time stack/average from beginning of session, for comparison
	also, accompanying session length distribution for context
	
	play with the bin width for nicer plot
	
Tim leaves Saturday, aims to have paper words by then

8.15

data gen working on 5-27-2016

fixed/improved the normalization
added bin width setting, for either normalization percent of session or time minutes

8.16

data gen update: 8-12-16

have session length distribution data - generated by sessions_adopt.py, dumped to sessions_adopt_data.csv

analyze_sessions.py should give us the rest of the plots/data, just need to dump to csv instead of only plotting

ran all the results, committed them up

new plots: like the adoption session plot, but slightly different
	instead of aligning on the first adoption event of each session, line up on ALL commits meeting criteria
	then stack/average activity surrounding each commit
	two versions:
		all non-adopt commits
		all adopt commits
this will give us a sense of activity surrounding both regular and non-adopt commits
go ahead and bin by 5 minute increments to make plots less squirrelly

Tim leaving Friday/Saturday, returning late Thursday

Tim will write up the basic crawl process, go ahead and edit that to be correct (see procedure_notes.txt)

95% confidence intervals for ALL averages (or standard error)

8.17

data gen: 10-28-2016

hmm - symmetric plots?

1 3 5 7 11 13 17

1 at center: 	0, 2, 4, 6, 10, 12, 16
3:				-2, 0, 2, 4, 8, 10, 14
5:				-4, -2, 0, 2, 6, 8, 12
7:				-6, -4, -2, 0, 4, 6, 10
11:				-10, -8, -6, -4, 0, 2, 6
13:				-12, -10, -8, -6, -2, 0, 4
17 at center: 	-16, -14, -12, -10, -6, -4, 0

stack at 0
						0	2	4	6	10	12	16
					-2	0	2	4	8	10	14
				-4	-2	0	2	6	8	12
			-6	-4	-2	0	4	6	10
		-10	-8	-6	-4	0	2	6
	-12	-10	-8	-6	-2	0	4
-16	-14	-12	-10	-6	-4	0

hmm - looks right to me?

try different - see Slack for notes

8.18

data gen: 1-23-17 (woohoo!)

altered sessions_adopt.py to compute average adoption time as % of session:
	Average adopt time as percentage of session: 13.94559333795975

okay, so what do we do with that?
	find regular non-adopt commits at approximately the same location, and stack them up
	probably also should stack ALL adoption commits, as opposed to just the first ones in each session
so... need new code I guess

stacking the adoptions done, not too hard

stacking the non-adopt ones at approx the same time is a bit more difficult, because have to step through the entire session to know it's length

8.19

data gen: 4-19-2017

back to code - stack the non-adopt commits that occur around the average adoption point

hmm, seems to be working - but maybe still kind of symmetric?
	do we have to pick a max of one non-adopt commit per session - whichever is closest to the avg adopt time?
	or rather, max of one from each session, but still must fall within the window
	
Processing data_files/user_commits/-1_commits.pkl
User -5 made 43 commits (1 adoption commits) adopting 1 libraries
Processed 9435270 commits and 36341 users
    175641 libraries adopted in 18440 adoption commits

Stacked 3283582 non-adoption commits for comparison
[-360, -355, -350, -345, -340, -335, -330, -325, -320, -315, -310, -305, -300, -295, -290, -285, -280, -275, -270, -265, -260, -255, -250, -245, -240, -235, -230, -225, -220, -215, -210, -205, -200, -195, -190, -185, -180, -175, -170, -165, -160, -155, -150, -145, -140, -135, -130, -125, -120, -115, -110, -105, -100, -95, -90, -85, -80, -75, -70, -65, -60, -55, -50, -45, -40, -35, -30, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330, 335, 340, 345, 350, 355]

8.20

data gen: 7-18-2017

tried something different, results pretty much the same - so stick with the old methodology

8.23

data gen finished?!?!!!!!!!
	finished 29267000 commits, 2926700 commits in history
		161082 does not adopt xml.etree.ElementTree at: 2018-06-03 18:25:52
		170335 does not adopt shutil at: 2018-06-07 13:15:05
		170335 does not adopt collections.defaultdict at: 2018-06-07 13:15:05
	saved events for 6-2018

	need to go back and regen some stuff for all the data, instead of just up to a certain point
	
housekeeping time - clear out all the old code, flag things that need updating

TODO:
	update divide_commits_by_user.py - include empty_events?
	rerun above
	
	consider combining the various different session analysis stuff into one program

notes from meeting: 
	from YY - instead of any non-adopt commit, look at just non-adopt import commits and see if that looks different
	andplusalso, error lines/bars for context
	
8.24

decision - only consider/process data up through Feb 2018
	since cloned first repos in March, anything that month or after is not complete
	
short-circuit divide_commits_by_user.py to only process Jan and Feb of 2018

added new dump method to file_utils

next task: combine and clean up the session analysis programs

8.27

have commit delay results to look at - verify 9 hour max delay
	eh, let's have it plot for us so we don't have to do that separately - since plot is easiest way to choose max delay

also keep combining session analysis

sessions.py all fixed, now working on incorporating sessions_adopt.py analysis
	resume at post-processing - computing averages and dumping data
	remember to remove exits and breaks!

run both ways - submodule and not

activity plots/data tonight - be available

adoption criteria: if an added lib is in updated_lib but not in the user's quiver, then it must be an adoption

user-sorted commits have the following fields: ['adopted_libs', 'time', 'del_libs', 'add_libs', 'id', 'user', 'repo']

next: for each user: - DONE
	number of commits
	number of adoptions
	number of active repos
	number of repos adopted from

for each repo: - DONE
	number of commits
	number of adoptions
	number of active users
	number of users adopting

andplusalso:
	user/repo distributions of above - DONE
	distribution of # of times seen library before adopted by user
	distribution of time delay between see library and adopt
	
	user source of adoption - number of times adopted from?
	
rename commit_analysis_user.py before committing - DONE
	something about overall instead?
	
sessions.py running

todo:
	unaddressed items above
	roll in sessions_adopt.py?
	ACTIVITY PLOTS - around_commits.py
	
sessions.py finished:
	Processed 29060288 commits and 162926 users in 4569625 sessions
		380519 libraries adopted in 54579 commits across 52646 sessions

	Average adopt time as percentage of session (all sessions): 8.919659683301305
	Average adopt time (no single commit sessions): 19.63483527687755

8.28

want time from user seen to user adopted
	and also user source
	
promoter, adopter, delay, library
x, u, delta_t, lib
v, u, delta_t, lib
...

do that FIRST


for activity plot:
	stack ALL commits (symmetric okay)
	stack import commits (see if different)
	
time to make Tim's graph - what's the best way?
	need to stream commits in order, but also need to know what is adoption and what isn't - unless determine on the fly?
	certainly don't need all the features, but do need to know when user "saw" the library - and who they got it from
	
	but determining them on the fly takes sooooo long
	what I really want is a list of commits formatted like the user-divided commits, but all of them together
	
	let's try making that, and see where that gets us
	
augmented commits - same commit data as all_commits_by_year, but add commit id and adopted libs list to each commit
	save each month as separate file
	
need to turn this into list of adoption graph edges: source (promoter), adopter, delay (between promoter commit and adoption), and library
	let's throw in repo too, just for good measure
	
code is ready - waiting on augmented_commits to finish, then run adopt_graph.py

crap - labelled augmented_commit files with wrong month - off by one error
	ie, 2018-01 is actually 2017-12
fixed now

adopt_graph.py finished
	Processed 29060288 commits, created 32363610 adoption edges
surprise... that's a big file! almost 3 GB

next: distribution of adoption delays

takes about 5 minutes to read entire csv - is streaming faster?
	much faster, more like 1.5 minutes
	
generate ALL the distributions
	number of times adopted from - DONE
	number of adoption edges (adopter) - DONE
	number of unique repos user adopts from - DONE
	number of unique repos user adopted from in (ie, how wide is their source reach?) - DONE

sample 1000 users, plot indegree vs outdegree - DONE
	probably just count for all, then sample

adoption graph edges:
	u -> v
	u commits first, v adopts lib from u
	
adopt_delay_analysis.py sample output:
	Sampled 1000 users from 50570 for scatter plot
	Processed 32363610 adoption edges
along with ALL the output files

around_commit.py STILL running - hopefully finish soon-ish

filtering adopt edges: ~2 minutes

hmmm... around_commit.py is bogging down on a user with 262,468 commits - ouch
	must be recent commits, since it didn't take this long before
let's just skip this user for now, because really?

nope - skip ALL users with more than 10K commits
	finished - YAY
	
# adoptions vs StackOverflow - top-level only
	# adoptions all-time
	maybe get a few SO stats - total # posts, total # views? something like that - yes, exactly these
	maybe get total commits as well? just in case
start there, can add more later
	
8.29

StackOverflow v adoption run finished - way faster than I thought

also want: time on x-axis, commits on y-axis
	plot additions, deletions, and adoptions on same plot
	see if some pattern
	for all libraries, or for a particular one
	also some SO data for comparison
Tim suggested 3 libs from different regions of stackoverflow v adoption plot: middle, top left, bottom right
	
hmmm... Tim has a 324,464 number for unique libraries adopted - where did that come from?
	let's check against adopt graph edges, since that's where he first mentions it
my quick set count says:
	found 93194 unique libraries adopted
which agrees with the by-month analysis total

count of top-level libs says: 279212
	these are just ANY libraries - added, deleted, or adopted
	
counting in adopt_graph.py (sets):
Processed 29060288 commits, created 32363610 adoption edges
   found 93194 unique libraries adopted
   found 162926 unique users in all commits
   found 38433 unique users adopting
   found 50570 users in adoption graph

counting in month_commit_analysis.py (again, sets):
	says 2,169,838 unique libraries seen (added, deleted, or adopted) across all user commits
	
******

prediction stuff	
	
send Tim list of features
yay downsampling - get adoption ratio
	downsampling rate as parameter
	
list of things:
	
first, predict/classify one month with many different training data periods
	1, 2, 3, 4, 5, 6, ... 12 months, 18 months, 24 months, etc
	
parameter sensitivity - pick classifier configuration		
	
for a few different test months:
	days after training (1-31) on x, AUC on y 
		
feature oblation - all the different feature sets

******

technically, user maybe didn't "see" libs in repo before they committed for the first time - but if they looked through the code they did
	need to let it ride, because otherwise features are junk
seems okay to me! and makes adopt_graph job easier
(letting it go)


adopt_graph update notes:

currently tracking last_interaction[user][repo] - probably don't need?
tracking sources as history[repo][lib] - but need global history now, not just repo-specific

keep active_repos[user] - set of repos user has committed to
then do history[lib][repo] - list of tuples of user, time, repo, commit id - in time-sorted order, because append

adopted libs = user's adopted libraries from this commit
user_repos = active_repos[user]
for lib in adopted libs:
	lib_history = history[lib]
	for repo in user_repos:
		for potential source in lib_history[repo]:
			generate adoption edge
			
sure?

could me multiple promotion edges between the same set of users (ie, user a commits lib l more than once before user b adopts) but I don't think that's a problem
	
oh, and there's probably a few features we need to throw out from prediction - anything adoption-related no longer valid

TODO:
	prediction stuff!!!!
	maybe time x, other stuff y plots - if time
	start combing through and verifying paper
	abstract update?
	
8.30

magic testing command

cd ../../data/rkrohn/libraryadoption

STR="$(date +"%F_%T")" && time python3 new_classify.py "predict_tests/results_$STR" CUPLS > "predict_tests/test_$STR.log" &

8.31

run ALL the things

consider doing:

also want: time on x-axis, commits on y-axis
	plot additions, deletions, and adoptions on same plot
	see if some pattern
	for all libraries, or for a particular one
	also some SO data for comparison
Tim suggested 3 libs from different regions of stackoverflow v adoption plot: middle, top left, bottom right

9.1

another thought - what if we train 7 separate classifiers, one for each day of the week, and then test each instance on the right one based on commit date?
	might matter if day of week has any impact
compare separate day-of-week classifiers against one trained on same period

9.3

predict 25 weeks (175 days) from random date, train 1 month, compute accuracy by week

classify_by_week.py

added_v_adopted.py
	Processed 29060288 commits, found 512097 commits with adoption
each row is # libs added, # libs deleted, # libs adopted
results/commit_scatter.csv - all commits
results/commit_scatter_adopt_only.csv - commits with at least one adopted library
results/commit_scatter_50K.csv - 50K random commits
results/commit_scatter_adopt_only_50K.csv - 50K random adoption commits

9.4

cleaning up the paper - weee!

let's go with Python and StackOverflow - capitalization



