procedure

1. Scrape list of repos to clone from GitHub, using github_script.py

	Fetch first 1000 Python repos, as determined by GitHub (seems to be based on popularity - stars or watches)
	
	Get all contributors to those 1000 repos.
	
	Get all Python repositories owned by those contributors.
	
	This produces lists of:
		157778 repos
		45476 contributors
		
	Now, add all contributors to those ~158K repos.
	
	Add all Python repos owned by that larger list of contributors.
	
	This yields a final list of:
		259923 repositories
		89311 users
		
	Input files: none
	Output files: located in github_files directory
		github_all_contrib.json    		list of all GitHub users contributing to scraped repositories  
		github_finished_users.json		list of users for which owned Python repos have been fetched (program-specific bookmark, not used later)
		github_all_repos.json        		list of all Python repositories scraped - used by clone program later
		github_repo_to_contrib.json		mapping of repositories to contributing users
		github_user_to_repo.json		mapping of GitHub users to repos they contribute to
		github_bad_users.json

2. Clone repositories, cleanup files, and extract raw commit data using github_clone_repos.py

	For each repo in github_all_repos.json, perform the following procedure:
	
	Clone the repository to repo_clones directory. Each repository will end up in a folder named <repo name>_<GitHub user owner>.
		
	Crawl the repository file structure to delete any unecessary files to save disk space. All that remains are .py (or similar extension) files and any files containing a valid import statement. The .git directory is unaffected.
		
	Do a first-pass search of commits, creating a file in the commit_data folder. All git diffs are searched for lines beginning with "import" or "from". These lines, along with the corresponding commit date, author name, and author email, are logged in that repository's commit data file.
	
	Final results: 259690 repositories cloned
	
	Input files:
		github_files/github_all_repos.json			list of all repositories to clone
	Output files: 
		github_files/github_fail_clone.txt			list of repositories that failed to clone (not used later)
		repo_clones/<repo name>__<owner GitHub id>/		one for each cloned repository
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
		
3. Build a list of users from the commit data, since there is no (easy) way to directly correlate GitHub users with git data. Uses build_user_list.py to create list, check_user_ids.py to run a quick assessment of mappings.

	For each commit in the raw files in commit_data, use the name and email address of the commit author to build a list of arbitrary user ids. Each unseen author name and email pair is assigned a new user id. If one of the identifiers, but not both, has already been seen, we assign the known user id to the unknown identifier. If both the name and email have been seen, but have conflicting user ids, combine the two users (along with all names and emails) into a single user.

	A few common, but unhelpful names and emails are not used for identifying users:
		names: "Unknown", "", "root", "(no author)", "unknown":
		emails: "none@none", "", "unknown", "Unknown":
	These were identified using commit_author_freq.py, which produces a sorted list of names/emails by frequency across all commits. Visual inspection of the top results revealed these unhelpful/standard identifiers.

	The optional check_user_ids.py reads the finished user list to compute the total number of users, the number of users with multiple names/emails, and the maximum number of names/emails for any single user.

	commit_author_freq.py
	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
	Output files: 
		data_files/author_email_freq.json		list of author emails and frequencies, sorted by frequency
		data_files/author_name_freq.json		list of author names and frequencies, sorted by frequency

	build_user_list.py produces:

		found 163677 users
		530 mystery commits (not necessarily import commits)
		72 repos affected
		final user list saved to name_to_userid.json and email_to_userid.json

	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
	Output files: 
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id


	check_user_ids.py produces:

		found 160105 name users
		found 162852 email users
		found 163677 total users
		max email addresses per user: 739
		number of users with multiple email addresses: 40286
		max names per user: 777
		number of users with multiple names 29528
		
	Input files:
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id
	Output files: 
		none		
		
4. Parse raw commit data for each repo into a more usable format using parse_commits.py.

	For each raw file in the commit_data directory, read in all commits to that repo and convert them to a file in the parsed_commit_data directory.

	For each commit, convert git author name and email to user id based on existing mapping files (from step 3). Preserve string UTC timestamp. 

	Parses each library import change to create a list of libraries imported and deleted for each commit. Valid import statements are located using a regex to ensure only true libraries are extracted. Depending on the current package_type flag in package_type.py, submodules are either preserved or stripped. Each commit written to a repo's log file has the following form:

	[
        	10890,
        	"1392579553",
        	{
            	"+": [
                	"os.path.join",
                	"os.path.dirname",
                	"setuptools.setup",
                	"setuptools.find_packages"
            	],
            	"-": [
                	"setuptools.setup"
            	]
        	}
    	]

	Note the "+" and "-" lists may not exist (absent dictionary keys) if the commit did not contain any import statements, since we want all commits represented.

	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id
	Output files: 
		parsed_commit_data/<repo_name>__<owner Github id>.log	single processed commit file for each repository
	
5. Compile all commits, across all repos, into a single json file using compile_all_commits.py

	Script reads each repo's log file in parsed_commit_data	and further processes the commit data: adds human-readable dictionary keys to all fields, converts UTC timestamp to integer, and adds field for repository name. All commits for all repos are saved to the same all_commits_<package_type>.json file. Each commit takes the following form:

	{
        	"repo": "active__fengmk2",
        	"add_libs": [
            		"urlparse.urlparse",
            		"urlparse.parse_qs",
            		"cgi.parse_qs"
        	],
        	"user": 48374,
        	"del_libs": [
            		"urlparse.parse_qs",
            		"urlparse.urlparse"
        	],
        	"time": 1306263191
    	}

	Note the add_libs and del_libs lists may be empty, since this file contains all commits, not only import commits.

	Input files:
		parsed_commit_data/<repo_name>__<owner Github id>.log	single processed commit file for each repository
	Output files: 
		data_files/all_commits_<package_type>.json	list of ALL commits (not just imports) across all repositories

		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		

