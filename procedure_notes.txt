procedure

1. Scrape list of repos to clone from GitHub, using github_script.py

	Fetch first 1000 Python repos, as determined by GitHub (seems to be based on popularity - stars or watches)
	
	Get all contributors to those 1000 repos.
	
	Get all Python repositories owned by those contributors.
	
	This produces lists of:
		157778 repos
		45476 contributors
		
	Now, add all contributors to those ~158K repos.
	
	Add all Python repos owned by that larger list of contributors.
	
	This yields a final list of:
		259923 repositories
		89311 GitHub users
		
	Input files: none
	Output files: located in github_files directory
		github_all_contrib.json    		list of all GitHub users contributing to scraped repositories  
		github_finished_users.json		list of users for which owned Python repos have been fetched (program-specific bookmark, not used later)
		github_all_repos.json        		list of all Python repositories scraped - used by clone program later
		github_repo_to_contrib.json		mapping of repositories to contributing users
		github_user_to_repo.json		mapping of GitHub users to repos they contribute to
		github_bad_users.json

2. Clone repositories, cleanup files, and extract raw commit data using github_clone_repos.py

	For each repo in github_all_repos.json, perform the following procedure:
	
	Clone the repository to repo_clones directory. Each repository will end up in a folder named <repo name>_<GitHub user owner>.
		
	Crawl the repository file structure to delete any unecessary files to save disk space. All that remains are .py (or similar extension) files and any files containing a valid import statement. The .git directory is unaffected.
		
	Do a first-pass search of commits, creating a file in the commit_data folder. All git diffs are searched for lines beginning with "import" or "from". These lines, along with the corresponding commit date, author name, and author email, are logged in that repository's commit data file.
	
	Final results: 259690 repositories cloned
	
	Input files:
		github_files/github_all_repos.json			list of all repositories to clone
	Output files: 
		github_files/github_fail_clone.txt			list of repositories that failed to clone (not used later)
		repo_clones/<repo name>__<owner GitHub id>/		one for each cloned repository
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
		
3. Build a list of users from the commit data, since there is no (easy) way to directly correlate GitHub users with git data. Uses build_user_list.py to create list, check_user_ids.py to run a quick assessment of mappings.

	For each commit in the raw files in commit_data, use the name and email address of the commit author to build a list of arbitrary user ids. Each unseen author name and email pair is assigned a new user id. If one of the identifiers, but not both, has already been seen, we assign the known user id to the unknown identifier. If both the name and email have been seen, but have conflicting user ids, combine the two users (along with all names and emails) into a single user.

	A few common, but unhelpful names and emails are not used for identifying users:
		names: "Unknown", "", "root", "(no author)", "unknown":
		emails: "none@none", "", "unknown", "Unknown":
	These were identified using commit_author_freq.py, which produces a sorted list of names/emails by frequency across all commits. Visual inspection of the top results revealed these unhelpful/standard identifiers.

	The optional check_user_ids.py reads the finished user list to compute the total number of users, the number of users with multiple names/emails, and the maximum number of names/emails for any single user.

	commit_author_freq.py
	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
	Output files: 
		data_files/author_email_freq.json		list of author emails and frequencies, sorted by frequency
		data_files/author_name_freq.json		list of author names and frequencies, sorted by frequency

	build_user_list.py produces:

		found 170413 users
		532 mystery commits (not necessarily import commits)
		74 repos affected
		final user list saved to name_to_userid.json and email_to_userid.json

	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
	Output files: 
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id

	check_user_ids.py produces:

		found 166793 name users
		found 169354 email users
		found 170413 total users
		max email addresses per user: 741
		number of users with multiple email addresses: 41069
		max names per user: 781
		number of users with multiple names 30030
		
	Input files:
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id
	Output files: 
		none		
		
4. Parse raw commit data for each repo into a more usable format using parse_commits.py.

	For each raw file in the commit_data directory, read in all commits to that repo and convert them to a file in the parsed_commit_data directory.

	For each commit, convert git author name and email to user id based on existing mapping files (from step 3). Preserve string UTC timestamp. 

	Parses each library import change to create a list of libraries imported and deleted for each commit. Valid import statements are located using a regex to ensure only true libraries are extracted. Depending on the current package_type flag in package_type.py, submodules are either preserved or stripped. Each commit written to a repo's log file has the following form:

	[
        	10890,
        	"1392579553",
        	{
            	"+": [
                	"os.path.join",
                	"os.path.dirname",
                	"setuptools.setup",
                	"setuptools.find_packages"
            	],
            	"-": [
                	"setuptools.setup"
            	]
        	}
    	]

	Note the "+" and "-" lists may not exist (absent dictionary keys) if the commit did not contain any import statements, since we want all commits represented.

	Input files:
		commit_data/<repo_name>__<owner GitHub id>.log		list of raw import commits for each repo
		data_files/name_to_id.json		mapping of name to integer user id
		data_files/email_to_id.json		mapping of email to integer user id
	Output files: 
		parsed_commit_data/<repo_name>__<owner Github id>.log	single processed commit file for each repository
	
5. Compile all commits, across all repos, into a single json file using compile_all_commits.py

	Script reads each repo's log file in parsed_commit_data	and further processes the commit data: adds human-readable dictionary keys to all fields, converts UTC timestamp to integer, and adds field for repository name. All commits for all repos are saved to the same all_commits_<package_type>.json file. Each commit takes the following form:

	{
        	"repo": "active__fengmk2",
        	"add_libs": [
            		"urlparse.urlparse",
            		"urlparse.parse_qs",
            		"cgi.parse_qs"
        	],
        	"user": 48374,
        	"del_libs": [
            		"urlparse.parse_qs",
            		"urlparse.urlparse"
        	],
        	"time": 1306263191
    	}

	Note the add_libs and del_libs lists may be empty, since this file contains all commits, not only import commits.

	Script output:
		results saved to data_files/all_commits_SUB.json
    			586 commits without timestamp (not included in compiled list)
		29268548 commits total

	Input files:
		parsed_commit_data/<repo_name>__<owner Github id>.log	single processed commit file for each repository
	Output files: 
		data_files/all_commits_<package_type>.json	list of ALL commits (not just imports) across all repositories
		
6. Sort the list of all commits by time, allowing for streaming processing of data. Also chunk the data into separate files by commit year to allow for easier model training and testing. Uses script sort_commits.py

	Script output:
		Preserving submodules
		reading in commit data...
		read in 29268548 commits
		sorting commits...
		saving sorted commits...
		29268548 sorted commits saved to data_files/all_commits_SUB_sorted.json
		earliest commit in 1969
		latest commit in 5586
    			2050 : 1 commits
    			2070 : 4 commits
    			overflow : 2 commits
    			1969 : 829 commits
    			1970 : 21 commits
    			1973 : 28 commits
    			1979 : 2 commits
    			1980 : 13 commits
    			1985 : 3 commits
    			1986 : 1 commits
    			1987 : 2 commits
    			1990 : 2523 commits
    			1991 : 10680 commits
    			1992 : 15064 commits
    			1993 : 6936 commits
    			1994 : 13958 commits
    			1995 : 30126 commits
    			1996 : 37284 commits
    			1997 : 53766 commits
    			1998 : 73004 commits
   			 1999 : 62020 commits
    			2000 : 120050 commits
    			2001 : 181902 commits
    			2002 : 187556 commits
    			2003 : 208286 commits
    			2004 : 234022 commits
    			2005 : 307315 commits
    			2006 : 521339 commits
    			2007 : 787204 commits
    			2008 : 1012142 commits
    			2009 : 1262158 commits
    			2010 : 1848185 commits
    			2011 : 2459750 commits
    			2012 : 3160030 commits
    			2013 : 3590287 commits
    			2014 : 3340657 commits
    			2015 : 3328950 commits
    			2016 : 2901661 commits
    			2017 : 2986646 commits
    			2018 : 524122 commits
    			2019 : 3 commits
    			2021 : 1 commits
    			2031 : 6 commits
    			2037 : 2 commits
    			2040 : 2 commits
    			2045 : 5 commits
		year-divided commits saved to data_files/all_commits_by_year/<year>_commits_SUB_sorted.json

	Input files:
		data_files/all_commits_<package_type>.json	list of ALL commits across all repos
	Output files: 
		data_files/all_commits_<package_type>_sorted.json	sorted list of all commits
		data_files/all_commits_by_year			directory containing one file per list of commits


Steps to writeup/update:

find_adoptions
adoption_analysis 
adoption_graph.py 
	
		
		
		
		
		
		
		
		
		
		
		
		
		
		

